{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Data Test Notebook\n",
    "\n",
    "The vast majority of this notebook is based off implementing the examples made available by the ray data [getting started](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) docs. \n",
    "\n",
    "\n",
    "\n",
    "### What kinds of things should I use Ray Datasets for? \n",
    "\n",
    "According to their docs, Ray is, \"designed to load and pre-process data for distributed ML training pipelines...Ray Datasets is not intended as a replacement for more general data processing systems\"[[1]]. Its purpose is only to serve as a \"last mile\" distributed data processing tool. Therefore it is designed with the following 3 use cases in mind. \n",
    "\n",
    "* Last Mile Processing\n",
    "* Parallel Batch Inference\n",
    "* ML Training Ingest (Distributed training)\n",
    "\n",
    "Below we will attempt to evaluate Ray for these different types of use cases. \n",
    "\n",
    "_Note: current testing / evaluation done on a local PC with 32GB memory. This will need to be scaled down to work on ODH with current pod resource sizes i think._ \n",
    "\n",
    "[1]: https://docs.ray.io/en/master/data/faq.html#what-should-i-use-ray-datasets-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.aggregate import Mean, Std\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from ray.util import connect as ray_connect\n",
    "from ray.util import disconnect as ray_disconnect\n",
    "from ray.util.client import ray as rayclient\n",
    "\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to our remote ray cluster if we're on an ODH notebook image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "if os.environ.get('RAY_CLUSTER') is not None:\n",
    "    if rayclient.is_connected():\n",
    "        ray_disconnect()\n",
    "\n",
    "    ray_connect('{ray_head}:10001'.format(ray_head=os.environ['RAY_CLUSTER']))\n",
    "    print(f\"connected to {os.environ.get('RAY_CLUSTER')}\") \n",
    "else:\n",
    "    print(\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Ray Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to test the capabilities of this Ray data tool, we are going to need a reasonably sized example data set. Let's create a CSV file that's almost 1GB and save it to our current file system.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"tmp/output\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.range(100000000)\n",
    "    print(\"writing file\")\n",
    "    ds.repartition(1).write_csv(\"tmp/output\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"file exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our \"BIG\" dataset, let's read it in with Ray vs vanilla pandas, run some basic data transformations and compare each's memory foot print.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.listdir(\"tmp/output/\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1699.57 MiB, increment: 1499.58 MiB\n",
      "CPU times: user 8.82 s, sys: 1.2 s, total: 10 s\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line = pd.read_csv(f\"tmp/output/{file}\")\n",
    "ds_line.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2425.41 MiB, increment: 1461.45 MiB\n",
      "CPU times: user 8.36 s, sys: 546 ms, total: 8.91 s\n",
      "Wall time: 9.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line = pd.read_csv(f\"tmp/output/{file}\")\n",
    "ds_line.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 964.46 MiB, increment: 0.00 MiB\n",
      "CPU times: user 104 ms, sys: 49.7 ms, total: 154 ms\n",
      "Wall time: 287 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 12588.46 MiB, increment: 11624.00 MiB\n",
      "CPU times: user 37.2 s, sys: 5.36 s, total: 42.5 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_line.applymap(lambda x: x *2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4170.55 MiB, increment: 3204.82 MiB\n",
      "CPU times: user 1.81 s, sys: 322 ms, total: 2.13 s\n",
      "Wall time: 2.27 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    value\n",
       "6       6\n",
       "7       7\n",
       "8       8\n",
       "9       9\n",
       "10     10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit \\\n",
    "ds_line = ds_line[ds_line[\"value\"] > 5]\n",
    "ds_line.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 16:12:52,333\tINFO services.py:1462 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 228.02 MiB, increment: 29.05 MiB\n",
      "Dataset(num_blocks=1, num_rows=None, schema={value: int64})\n",
      "CPU times: user 302 ms, sys: 119 ms, total: 421 ms\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "### Wokers don't have PVC access...So this won't work like locally \n",
    "%%time\n",
    "%memit ds_dst = ray.data.read_csv(f\"tmp/output/{file}\")\n",
    "print(ds_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 925.93 MiB, increment: 695.38 MiB\n",
      "CPU times: user 4.47 s, sys: 363 ms, total: 4.83 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.take(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 1/1 [02:08<00:00, 128.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 678.05 MiB, increment: 2.24 MiB\n",
      "CPU times: user 2.11 s, sys: 413 ms, total: 2.52 s\n",
      "Wall time: 2min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.map_batches(lambda df:  df.applymap(lambda x: x *2), batch_format='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 1/1 [01:26<00:00, 86.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 229.45 MiB, increment: 0.86 MiB\n",
      "CPU times: user 1.57 s, sys: 283 ms, total: 1.85 s\n",
      "Wall time: 1min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'value': 6},\n",
       " {'value': 7},\n",
       " {'value': 8},\n",
       " {'value': 9},\n",
       " {'value': 10},\n",
       " {'value': 11},\n",
       " {'value': 12},\n",
       " {'value': 13},\n",
       " {'value': 14},\n",
       " {'value': 15}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ds_dst.map_batches(lambda df: df[df[\"value\"] > 5], batch_format=\"pandas\")\n",
    "ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all of the above cells looks to leave you with about a 20Gb memory load... May have to reset the kernel to move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have a distributed dataset? not 1 file and partition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset\n",
      "peak memory: 683.82 MiB, increment: 0.95 MiB\n",
      "writing file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Write Progress: 100%|██████████| 200/200 [01:43<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"tmp/output_dist\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.range(100000000)\n",
    "    print(\"writing file\")\n",
    "    ds.write_csv(\"tmp/output_dist\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"files exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 686.88 MiB, increment: 0.17 MiB\n",
      "Dataset(num_blocks=200, num_rows=None, schema={value: int64})\n",
      "CPU times: user 137 ms, sys: 35.2 ms, total: 172 ms\n",
      "Wall time: 476 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ray.data.read_csv(f\"tmp/output_dist/\")\n",
    "print(ds_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 941.44 MiB, increment: 254.50 MiB\n",
      "CPU times: user 4.53 s, sys: 498 ms, total: 5.03 s\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.take(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [02:03<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 697.15 MiB, increment: 1.21 MiB\n",
      "CPU times: user 2.94 s, sys: 483 ms, total: 3.42 s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst.map_batches(lambda df:  df.applymap(lambda x: x *2), batch_format='pandas') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [01:26<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 697.88 MiB, increment: 0.73 MiB\n",
      "CPU times: user 2.64 s, sys: 426 ms, total: 3.07 s\n",
      "Wall time: 1min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'value': 6},\n",
       " {'value': 7},\n",
       " {'value': 8},\n",
       " {'value': 9},\n",
       " {'value': 10},\n",
       " {'value': 11},\n",
       " {'value': 12},\n",
       " {'value': 13},\n",
       " {'value': 14},\n",
       " {'value': 15}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds_dst = ds_dst.map_batches(lambda df: df[df[\"value\"] > 5], batch_format=\"pandas\")\n",
    "ds_dst.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have evaluated 3 scenarios: Using vanilla pandas with a single dataset, using Ray with a single dataset and using Ray with a distributed dataset for a number of different operations. \n",
    "\n",
    "Our generated datasets are 100,000,000 rows long and 1 column wide, consisting only of integers. Below we have recorded the timing and memory results for loading the data, retrieving a slice (subset), applying the square function to each element and applying a filter to the dataset along with the total time taken to perform each step and the memory still in use after the entire set of operations ran. \n",
    "\n",
    "#### Ray vs Pandas performance results\n",
    "\n",
    "|                   |  Load      | Slice     |  Square     |  Filter   | Total Change | \n",
    "|-------------------|------------|-----------|-------------|-----------|--------------|\n",
    "|Pandas             | 10s, 800mb | 1s, 0mb   | 43s, 0mb    | 2s, 500mb | 56s, 1600mb  |\n",
    "|Ray (single block) | 9s, 1600mb | 5s,400mb  | 128s,2400mb | 86s,0mb   | 228s, 4400mb |\n",
    "|Ray (multi block)  | 1s, 1000mb | 5s, 400mb | 35s, 1100mb | 14s, 0mb  | 55s, 2500mb  |\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "From the table above we can see that using Ray data without dividing our dataset object into a reasonable number of blocks performs quite poorly. Its by far the slowest approach for the operations above and uses the most memory overall. \n",
    "For a smallish dataset like we are using here (~1Gb) vanilla pandas still works fairly well, however, it is still running as a single process and is not taking full advantage of the available resources. \n",
    "With the Ray Dataset divided into 200 blocks we get (in some cases) faster times than pandas with only about 1Gb more memory required. Furthermore, this approach maximizes use of the available resource on the machine. \n",
    "\n",
    "We are also able to convert Ray (single block) to Ray (multi block) and get the same increased performance by running a `ds.repartition(200)` command on our dataset. However, it is a somewhat expensive operation and should be avoided if possible.  \n",
    "\n",
    "\n",
    "_note: These are the experimental results on an 8 core laptop with 32Gb Memory and should be repeated on an OPF cluster._\n",
    "\n",
    "_note 2: The memory values recorded above from %memit did not seem to accurately capture the amount of memory used by the multiple Ray processes, so the chart reflects total usage from machine while running above code and not the %memit values._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: ML Preprocessing \n",
    "\n",
    "In this section we will mostly follow the [\"dataset ml preprocessing\"](https://docs.ray.io/en/latest/data/examples/big_data_ingestion.html) section of the Ray data docs to evaluate some of the \"last mile\" type of processing we'd want to use Ray for in a machine learning pipeline. Specifically we will perform the following 3 types of operations:\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Aggregation and scaling\n",
    "3. Random Shuffle\n",
    "\n",
    "The first thing we need to do is create a slightly more complex data set, one that has 3 columns with proper column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files exists\n"
     ]
    }
   ],
   "source": [
    "# make a multi-column data set\n",
    "if os.path.exists(\"tmp/output_multi_col\") == False:\n",
    "    print(\"creating dataset\")\n",
    "    %memit \\\n",
    "    ds = ray.data.from_items([{\"A\":i%3,\"B\":i * 2,\"C\":i * 3} for i in range(20000000)])\n",
    "    print(\"writing file\")\n",
    "    ds.write_csv(\"tmp/output_multi_col\")\n",
    "    del ds\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"files exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "Cool, let's encapsulate all the data cleaning steps we want to perform on our data into a single function. This is good practice in general, but will also let us pass this function to ray to be run in parallel on our dataset. \n",
    "\n",
    "All the moves below are arbitrary and selected just to show what's possible :)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(df: pd.DataFrame):\n",
    "    # Drop nulls.\n",
    "    df = df.dropna(subset=[\"A\"])\n",
    "    # Add new column.\n",
    "    df[\"new_col\"] = df[\"A\"] - 2 * df[\"B\"] + df[\"C\"] / 3\n",
    "    # Transform existing column.\n",
    "    df[\"A\"] = 2 * df[\"A\"] + 1\n",
    "    # Drop column.\n",
    "    df.drop(columns=\"B\", inplace=True)\n",
    "    # Re-add column \n",
    "    df[\"B\"] = df[\"C\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our new dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 15:53:52,033\tINFO services.py:1462 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 230.08 MiB, increment: 32.63 MiB\n",
      "Dataset(num_blocks=200, num_rows=None, schema={A: int64, B: int64, C: int64})\n",
      "CPU times: user 297 ms, sys: 86.7 ms, total: 384 ms\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit ds = ray.data.read_csv(f\"tmp/output_multi_col/\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the transformations to our dataset in parallel on each block.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:13<00:00, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 255 ms, total: 2.05 s\n",
      "Wall time: 14.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 1, 'C': 0, 'new_col': 0.0, 'B': 0},\n",
       " {'A': 3, 'C': 3, 'new_col': -2.0, 'B': 3},\n",
       " {'A': 5, 'C': 6, 'new_col': -4.0, 'B': 6},\n",
       " {'A': 1, 'C': 9, 'new_col': -9.0, 'B': 9},\n",
       " {'A': 3, 'C': 12, 'new_col': -11.0, 'B': 12}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ds = ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND, for good measure, let's compare timing of loading our dataset and running our cleaning function using regular old pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1251.20 MiB, increment: 1010.46 MiB\n",
      "CPU times: user 4.85 s, sys: 1.48 s, total: 6.33 s\n",
      "Wall time: 6.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "files = os.listdir(\"tmp/output_multi_col\")\n",
    "files = [f\"tmp/output_multi_col/{file}\" for file in files]\n",
    "%memit ds_panda = pd.concat(map(pd.read_csv, files))\n",
    "ds_panda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 929 ms, sys: 268 ms, total: 1.2 s\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>new_col</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   C  new_col   B\n",
       "0  1   0      0.0   0\n",
       "1  3   3     -2.0   3\n",
       "2  5   6     -4.0   6\n",
       "3  1   9     -9.0   9\n",
       "4  3  12    -11.0  12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ds_panda = transform_batch(ds_panda)\n",
    "ds_panda.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations and Scaling\n",
    "\n",
    "Now let's looks at a few operations like getting the mean, std, and scaling our data set that require knowledge of the whole dataset making them a little more difficult to parallelize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [01:18<00:00,  2.56it/s]\n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29999998.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds.mean(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean(B)': 29999998.5, 'mean(C)': 29999998.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds.mean([\"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we'll run the same operations with pandas so we have something to compare our results to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "B    29999998.5\n",
       "C    29999998.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds_panda[[\"B\",\"C\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupBy Map: 100%|██████████| 200/200 [04:58<00:00,  1.49s/it]\n",
      "GroupBy Reduce: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 459 ms, total: 3.86 s\n",
      "Wall time: 5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean(B)': 29999998.5, 'std(B)': 17320508.50870145, 'mean(C)': 29999998.5, 'std(C)': 17320508.50870145, 'mean(new_col)': -29999997.50000005, 'std(new_col)': 17320508.508702107}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stats = ds.aggregate(Mean(\"B\"), Std(\"B\"), Mean(\"C\"), Std(\"C\"), Mean(\"new_col\"), Std(\"new_col\") )\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_standard_scaler(df: pd.DataFrame):\n",
    "    def column_standard_scaler(s: pd.Series):\n",
    "        s_mean = stats[f\"mean({s.name})\"]\n",
    "        s_std = stats[f\"std({s.name})\"]\n",
    "        return (s - s_mean) / s_std\n",
    "\n",
    "    cols = df.columns.difference([\"A\"])\n",
    "    df.loc[:, cols] = df.loc[:, cols].transform(column_standard_scaler)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map Progress: 100%|██████████| 200/200 [00:10<00:00, 19.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 1, 'C': -1.7320506776650724, 'new_col': 1.732050619929984, 'B': -1.7320506776650724},\n",
       " {'A': 3, 'C': -1.7320505044599959, 'new_col': 1.7320505044599332, 'B': -1.7320505044599959},\n",
       " {'A': 5, 'C': -1.7320503312549196, 'new_col': 1.7320503889898822, 'B': -1.7320503312549196},\n",
       " {'A': 1, 'C': -1.732050158049843, 'new_col': 1.7320501003147548, 'B': -1.732050158049843},\n",
       " {'A': 3, 'C': -1.7320499848447666, 'new_col': 1.732049984844704, 'B': -1.7320499848447666}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "ds = ds.map_batches(batch_standard_scaler, batch_format=\"pandas\")\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle\n",
    "\n",
    "When running ML training pipelines it is considered good practice to shuffle our training set at the beginning of each epoch. Let's look at a couple different ways we can shuffle our data with Ray.  \n",
    "\n",
    "* First we will shuffle the whole dataset once\n",
    "* Then we will shuffle it N times\n",
    "* Finally, we create a DatasetPipeline object that will shuffle each block when called in an iteration loop (like we would do for training) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 200/200 [00:07<00:00, 26.83it/s]\n",
      "Shuffle Reduce: 100%|██████████| 200/200 [00:21<00:00,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=200, num_rows=20000000, schema={A: int64, C: float64, new_col: float64, B: float64})\n",
      "CPU times: user 6.03 s, sys: 2.31 s, total: 8.34 s\n",
      "Wall time: 29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'A': 3, 'C': -1.113222541377082, 'new_col': 1.1132225413770425, 'B': -1.113222541377082},\n",
       " {'A': 5, 'C': -1.1175845380218135, 'new_col': 1.1175845957567994, 'B': -1.1175845380218135},\n",
       " {'A': 5, 'C': -1.1177726387348128, 'new_col': 1.117772696469799, 'B': -1.1177726387348128},\n",
       " {'A': 5, 'C': -1.1159144946749067, 'new_col': 1.1159145524098926, 'B': -1.1159144946749067},\n",
       " {'A': 3, 'C': -1.111170580836654, 'new_col': 1.1111705808366148, 'B': -1.111170580836654}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Shuffle once\n",
    "ds = ds.random_shuffle()\n",
    "print(ds)\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 200/200 [00:07<00:00, 28.19it/s]\n",
      "Shuffle Reduce: 100%|██████████| 200/200 [00:21<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.17 s, sys: 2.06 s, total: 8.23 s\n",
      "Wall time: 28.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetPipeline(num_windows=20, num_stages=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Shuffle N times\n",
    "ds.random_shuffle().repeat(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|██████████| 20/20 [08:13<00:00, 24.68s/it]\n",
      "Stage 0: 100%|██████████| 20/20 [08:13<00:00, 24.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 31.7 s, total: 2min 40s\n",
      "Wall time: 8min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>new_col</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11960</td>\n",
       "      <td>52.856811</td>\n",
       "      <td>-52.856812</td>\n",
       "      <td>52.856811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11972</td>\n",
       "      <td>-174.240306</td>\n",
       "      <td>174.240305</td>\n",
       "      <td>-174.240306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11886</td>\n",
       "      <td>-3.698579</td>\n",
       "      <td>3.698575</td>\n",
       "      <td>-3.698579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11946</td>\n",
       "      <td>-20.915122</td>\n",
       "      <td>20.915120</td>\n",
       "      <td>-20.915122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11970</td>\n",
       "      <td>0.412173</td>\n",
       "      <td>-0.412174</td>\n",
       "      <td>0.412173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>12090</td>\n",
       "      <td>39.352158</td>\n",
       "      <td>-39.352156</td>\n",
       "      <td>39.352158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>11960</td>\n",
       "      <td>-49.184004</td>\n",
       "      <td>49.184003</td>\n",
       "      <td>-49.184004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>12136</td>\n",
       "      <td>26.850452</td>\n",
       "      <td>-26.850448</td>\n",
       "      <td>26.850452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>12084</td>\n",
       "      <td>71.344830</td>\n",
       "      <td>-71.344828</td>\n",
       "      <td>71.344830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12188</td>\n",
       "      <td>-12.748882</td>\n",
       "      <td>12.748887</td>\n",
       "      <td>-12.748882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           A           C     new_col           B\n",
       "0      11960   52.856811  -52.856812   52.856811\n",
       "1      11972 -174.240306  174.240305 -174.240306\n",
       "2      11886   -3.698579    3.698575   -3.698579\n",
       "3      11946  -20.915122   20.915120  -20.915122\n",
       "4      11970    0.412173   -0.412174    0.412173\n",
       "...      ...         ...         ...         ...\n",
       "99995  12090   39.352158  -39.352156   39.352158\n",
       "99996  11960  -49.184004   49.184003  -49.184004\n",
       "99997  12136   26.850452  -26.850448   26.850452\n",
       "99998  12084   71.344830  -71.344828   71.344830\n",
       "99999  12188  -12.748882   12.748887  -12.748882\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# create a pipeline that trigger a random shuffle before each batch (epoch)\n",
    "ds = ds.repeat(num_epochs).random_shuffle_each_window()\n",
    "\n",
    "n = 0\n",
    "for i in ds.iter_batches():\n",
    "    n += len(i)\n",
    "n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so from the above, we can see how to use Ray to apply some common \"last mile\" data processing types of transformations to our dataset in a parallel fashion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Pipelines \n",
    "\n",
    "In this section we will mostly follow the examples from [\"pipelining-compute\"](https://docs.ray.io/en/latest/data/pipelining-compute.html) and [\"advanced-pipelines\"](https://docs.ray.io/en/latest/data/advanced-pipelines.html) from the Ray docs to demonstrate how and when to use \"DatasetPipelines\". \n",
    "\n",
    "According to the docs, \"Unlike Datasets, which execute all transformations synchronously, DatasetPipelines implement pipelined execution. This allows for the overlapped execution of data input (e.g., reading files), computation (e.g. feature preprocessing), and output (e.g., distributed ML training).\"\n",
    "\n",
    "We saw DatasetPipelines a bit in the earlier section for shuffling our data. Here will look into constructing slightly more complex pipelines. \n",
    "\n",
    "First things first; Let's build a small dataset we can convert into a DatasetPipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=200, num_rows=100000, schema=<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "base = ray.data.range(100000)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `.window()` to convert our Dataset into a DatasetPipeline with 10 blocks per window (20 windows for 200 blocks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 16:22:31,729\tINFO dataset.py:2649 -- Created DatasetPipeline with 20 windows: 0.04MiB min, 0.04MiB max, 0.04MiB mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=20, num_stages=2)\n"
     ]
    }
   ],
   "source": [
    "pipe = base.window(blocks_per_window=10)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define some functions we want to apply to our Data through the DatasetPipeline approach. We then use `pipe.map(func_N)` to add them to our pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(i):\n",
    "    return i+1\n",
    "\n",
    "def func2(i):\n",
    "    return i *2\n",
    "\n",
    "def func3(i):\n",
    "    return i%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetPipeline(num_windows=20, num_stages=5)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipe.map(func1)\n",
    "pipe = pipe.map(func2)\n",
    "pipe = pipe.map(func3)\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the DatasetPipeline is defined, we have to iterate over it for it to trigger the computations we've defined on it. To do that let's just run a quick for loop over data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 1: 100%|██████████| 20/20 [00:18<00:00,  1.11it/s]\n",
      "Stage 0: 100%|██████████| 20/20 [00:18<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_rows = 0\n",
    "for row in pipe.iter_batches():\n",
    "    num_rows += len(row) \n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now we know how to create, define and run DatasetPipelines with Ray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Large Scale ML Ingest Example\n",
    "\n",
    "Here we will go ahead an follow the [\"Big Data Ingestion\"](https://docs.ray.io/en/latest/data/examples/big_data_ingestion.html) example from the Ray docs. \n",
    "\n",
    "The goal here is to tie together everything above into a single demo that reflects a more _realistic_ scenario on how we would apply the Ray Data toolkit to a parallel and distributed machine learning use case.  \n",
    "\n",
    "First thing we will do is define a function called `create_shuffle_pipeline` that will turn our Dataset into a DatasetPipeline that will read in our data for each epoch, shuffle it and split it into equally sized shards for distributed training on multiple workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shuffle_pipeline(training_data_dir: str, num_epochs: int, num_shards: int):\n",
    "\n",
    "    return (\n",
    "        ray.data.read_csv(training_data_dir)\n",
    "        .repeat(num_epochs)\n",
    "        .random_shuffle_each_window()\n",
    "        .split(num_shards, equal=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will define our own remote `TrainingWorker` class that iterates over our shards during training. For simplicity we will simple `pass` our training step as we are focused on just the distributed data processing steps here (keep things simple).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, rank, shard):\n",
    "        self.rank = rank\n",
    "        self.shard = shard\n",
    "\n",
    "    def train(self):\n",
    "        for epoch, training_dataset in enumerate(self.shard.iter_epochs()):\n",
    "            # Following code emulates epoch based SGD training.\n",
    "            print(f\"Training... worker: {self.rank}, epoch: {epoch}\")\n",
    "            for i, batch in enumerate(training_dataset.iter_batches()):\n",
    "                # TODO: replace the code for real training.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the two key variables for this example, the number of Ray workers we'll use and the number of epochs to run. With the appropriate cluster resources, we can scale up our data ingest here by increasing the number of workers.\n",
    "\n",
    "According to the docs this whole process can be linearly scaled to arbitrarily large data sets (example is 500gb) by adding more nodes to our cluster and increasing our `NUM_TRAINING_WORKERS`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    NUM_TRAINING_WORKERS = 4\n",
    "    NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our DatasetPipeline called `splits` and instantiate our list of `TrainingWorkers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 278 ms, sys: 38.9 ms, total: 317 ms\n",
      "Wall time: 369 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/5 [00:00<?, ?it/s]=4106916)\u001b[0m \n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[Aor pid=4106916)\u001b[0m \n",
      "Stage 1:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A06916)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splits = create_shuffle_pipeline(f\"tmp/output_multi_col/\", NUM_EPOCHS, NUM_TRAINING_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 5.82 ms, total: 26.4 ms\n",
      "Wall time: 19.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_workers = [TrainingWorker.remote(rank, shard) for rank, shard in enumerate(splits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use, `ray.get` to train our remote training_workers in parallel! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:  20%|██        | 1/5 [00:17<01:08, 17.17s/it])\u001b[0m \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=4106916)\u001b[0m \n",
      "Stage 0:  40%|████      | 2/5 [00:26<00:37, 12.64s/it]\u001b[A0m \n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=4106916)\u001b[0m \n",
      "Stage 0:  60%|██████    | 3/5 [00:34<00:20, 10.38s/it]\u001b[A0m \n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4106999)\u001b[0m Training... worker: 0, epoch: 0\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107001)\u001b[0m Training... worker: 1, epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107002)\u001b[0m Training... worker: 2, epoch: 0\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107003)\u001b[0m Training... worker: 3, epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=4106916)\u001b[0m \n",
      "Stage 0:  80%|████████  | 4/5 [00:42<00:09,  9.55s/it]\u001b[A0m \n",
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.38s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4106999)\u001b[0m Training... worker: 0, epoch: 1\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107001)\u001b[0m Training... worker: 1, epoch: 1\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107003)\u001b[0m Training... worker: 3, epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.53s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.42s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:10<00:00, 10.64s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107002)\u001b[0m Training... worker: 2, epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=4106916)\u001b[0m \n",
      "Stage 0: 100%|██████████| 5/5 [00:50<00:00,  9.13s/it]\u001b[A0m \n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4106999)\u001b[0m Training... worker: 0, epoch: 2\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107003)\u001b[0m Training... worker: 3, epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107001)\u001b[0m Training... worker: 1, epoch: 2\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107002)\u001b[0m Training... worker: 2, epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.50s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(PipelineSplitExecutorCoordinator pid=4106916)\u001b[0m \n",
      "Stage 1: 100%|██████████| 5/5 [00:59<00:00,  9.81s/it]\u001b[A0m \n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.67s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.31s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.44s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:07<00:00,  7.57s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4106999)\u001b[0m Training... worker: 0, epoch: 3\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107001)\u001b[0m Training... worker: 1, epoch: 3\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107002)\u001b[0m Training... worker: 2, epoch: 3\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107003)\u001b[0m Training... worker: 3, epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TrainingWorker pid=4106999)\u001b[0m Training... worker: 0, epoch: 4\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107001)\u001b[0m Training... worker: 1, epoch: 4\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107003)\u001b[0m Training... worker: 3, epoch: 4\n",
      "\u001b[2m\u001b[36m(TrainingWorker pid=4107002)\u001b[0m Training... worker: 2, epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Stage 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 347 ms, total: 1.52 s\n",
      "Wall time: 44.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage 0: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ray.get([worker.train.remote() for worker in training_workers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "If you are looking at this cell and there are no error above, you know that Ray Data is working! "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e34450d332bd313b8f818cb5ed04e25933b13de9c0d7b662ddcaf48d79a536f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
