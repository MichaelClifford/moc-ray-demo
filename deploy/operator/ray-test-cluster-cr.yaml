kind: RayCluster
apiVersion: cluster.ray.io/v1
metadata:
  name: ray-test-cluster-1
spec:
  # The maximum number of workers nodes to launch in addition to the head node.
  maxWorkers: 5
  # The autoscaler will scale up the cluster faster with higher upscaling speed.
  # E.g., if the task requires adding more nodes then autoscaler will gradually
  # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
  # This number should be > 0.
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: head-node
  # Specify the allowed pod types for this ray cluster and the resources they provide.
  podTypes:
  - name: head-node
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: ray-test-cluster-1-head-
      spec:
        restartPolicy: Never
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: 'quay.io/erikerlandson/ray-client-server:use-ray-operator'
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ['/opt/ray/bin/run-ray-head' ]
          env:
            - name: RAY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: RAY_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  resource: requests.cpu
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: '1'
              memory: 1024Mi
            limits:
              cpu: '1'
              # The maximum memory that this pod is allowed to use. The
              # limit will be detected by ray and split to use 10% for
              # redis, 30% for the shared memory object store, and the
              # rest for application memory. If this limit is not set and
              # the object store size is not set manually, ray will
              # allocate a very large object store in each pod that may
              # cause problems for other pods.
              memory: 1024Mi
  - name: worker-nodes
    # Minimum number of Ray workers of this Pod type.
    minWorkers: 3
    # Maximum number of Ray workers of this Pod type. Takes precedence over minWorkers.
    maxWorkers: 5
    # User-specified custom resources for use by Ray 
    rayResources: {"Custom1": 1, "is_spot": 1}
    # Optional commands to run before starting the Ray runtime.
    setupCommands: 
      - pip install numpy # Example
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: ray-test-cluster-1-worker-
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: 'quay.io/erikerlandson/ray-client-server:use-ray-operator'
          command: [ '/opt/ray/bin/run-ray-worker' ]
          env:
            - name: RAY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: RAY_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  resource: requests.cpu
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: '1'
              memory: 1024Mi
            limits:
              cpu: '1'
              # The maximum memory that this pod is allowed to use. The
              # limit will be detected by ray and split to use 10% for
              # redis, 30% for the shared memory object store, and the
              # rest for application memory. If this limit is not set and
              # the object store size is not set manually, ray will
              # allocate a very large object store in each pod that may
              # cause problems for other pods.
              memory: 1024Mi
  # Commands to start Ray on the head node. You don't need to change this.
  # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
  headStartRayCommands:
      - 'echo "head start ray command invoked"'
  # Commands to start Ray on worker nodes. You don't need to change this.
  workerStartRayCommands:
      - 'echo "worker start ray command invoked"'
