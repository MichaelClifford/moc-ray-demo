kind: ConfigMap
apiVersion: v1
metadata:
  name: ray-cluster-template
data:
  rayHeadServiceTemplate: |
    apiVersion: v1
    kind: Service
    metadata:
      name: "ray-head-{{ user }}"
    spec:
      ports:
        # open question: are all these strictly necessary to define in this service?
        - name: client-server
          port: 50051
          targetPort: 50051
        - name: dashboard
          port: 8265
          targetPort: 8265
        - name: redis-primary
          port: 6379
          targetPort: 6379
        - name: redis-shard-0
          port: 6380
          targetPort: 6380
        - name: redis-shard-1
          port: 6381
          targetPort: 6381
        - name: object-manager
          port: 12345
          targetPort: 12345
        - name: node-manager
          port: 12346
          targetPort: 12346
      selector:
        ray-cluster-deployment: "ray-head-{{ user }}"
  rayHeadDeploymentTemplate: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: "ray-head-{{ user }}"
    spec:
      # Do not change this - Ray currently only supports one head node per cluster.
      replicas: 1
      selector:
        matchLabels:
          ray-cluster-deployment: "ray-head-{{ user }}"
      template:
        metadata:
          labels:
            ray-cluster-deployment: "ray-head-{{ user }}"
        spec:
          restartPolicy: Always
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          containers:
            - name: "ray-head-{{ user }}"
              image: "{{ ray_image }}"
              imagePullPolicy: Always
              command: [ '/opt/ray/bin/run-ray-head' ]
              # This volume allocates shared memory for Ray to use for its plasma
              # object store. If you do not provide this, Ray will fall back to
              # /tmp which cause slowdowns if is not a shared memory volume.
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
              env:
                - name: RAY_POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                # This is used in the ray start command so that Ray can spawn the
                # correct number of processes. Omitting this may lead to degraded
                # performance.
                - name: RAY_CPU_REQUEST
                  valueFrom:
                    resourceFieldRef:
                      resource: requests.cpu
              resources:
                requests:
                  cpu: 1
                  memory: "{{ head_memory_request }}"
                limits:
                  cpu: 1
  rayWorkerDeploymentTemplate: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: "ray-worker-{{ user }}"
    spec:
      # Change this to scale the number of worker nodes started in the Ray cluster.
      # non-string template parameters are currently not supported due to a bug
      # in the ODH JH launcher, so I'm just hard coding for now.
      # Also to-do: integrate the ray autoscaler
      replicas: 3
      selector:
        matchLabels:
          ray-cluster-deployment: "ray-worker-{{ user }}"
      template:
        metadata:
          labels:
            ray-cluster-deployment: "ray-worker-{{ user }}"
        spec:
          restartPolicy: Always
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          containers:
          - name: "ray-worker-{{ user }}"
            image: "{{ ray_image }}"
            imagePullPolicy: Always
            command: [ '/opt/ray/bin/run-ray-worker' ]
            volumeMounts:
              - mountPath: /dev/shm
                name: dshm
            env:
              - name: RAY_HEAD_NODE
                value: 'ray-head-{{ user }}'
              - name: RAY_POD_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.podIP
              - name: RAY_CPU_REQUEST
                valueFrom:
                  resourceFieldRef:
                    resource: requests.cpu
            resources:
              requests:
                cpu: 1
                memory: "{{ worker_memory_request }}"
              limits:
                cpu: 1
